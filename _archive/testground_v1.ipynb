{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projects = [ # total: 13 point clouds (including bbox splitting)\n",
    "    # {\"proj_name\": \"2023-08-28_FW_EingangBauing.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-03-22_FW_Koenigshuegel.FwfProj\", \"bboxes\" : []},                      # building\n",
    "    # {\"proj_name\": \"2024-04-05_FW_Westbahnhof_01.FwfProj\", \"bboxes\" : []},                   # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_02.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_03.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_04.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_05.FwfProj\", \"bboxes\" : []},                     # building\n",
    "    {\"proj_name\": \"2024-05-10_FW_RWTH_Zentrum_01.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Koenigstr.FwfProj\", \"bboxes\" : [0,2]},               # bridge (steel+stone)\n",
    "    # {\"proj_name\": \"2024-07-31_FW_Bruecke_Turmstr.FwfProj\", \"bboxes\" : []},                  # bridge (concrete/steel)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj\", \"bboxes\" : []},   # bridge (concrete)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Deltourserb.FwfProj\", \"bboxes\" : []},                # bridge (concrete)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj\", \"bboxes\" : [1]},             # bridge (concrete, steel, brick)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_RotheErde.FwfProj\", \"bboxes\" : []},                  # bridge (steel)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Rottstrasse.FwfProj\", \"bboxes\" : []},                # bridge (concrete)\n",
    "]\n",
    "val_projects = [ # total: 5 point clouds (including bbox splitting)\n",
    "    {\"proj_name\": \"2023-08-28_FW_EingangBauing.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_01.FwfProj\", \"bboxes\" : []},                   # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Koenigstr.FwfProj\", \"bboxes\" : [1]},               # bridge (steel+stone)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj\", \"bboxes\" : [0]},           # bridge (concrete, steel, brick)\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Turmstr.FwfProj\", \"bboxes\" : []},                  # bridge (concrete/steel)\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BBox:\n",
    "    \"\"\"Bounding box using the CloudCompare synthax\n",
    "    \"\"\"\n",
    "    def __init__(self, orientation:np.ndarray, width: np.ndarray) -> None:\n",
    "        # an easy way to retrieve these values is by using the cross-section tool in cloudcompare\n",
    "        # so cross-section -> advanced -> orientation -> to clipboard\n",
    "        self.orientation = orientation\n",
    "        # from the same menu copy and paste the values under 'width' \n",
    "        self.width = width\n",
    "        \n",
    "    def cutout(self, xyz:np.ndarray) -> np.ndarray:\n",
    "            # Inverse of the orientation matrix (rotation and translation)\n",
    "            orientation_inv = np.linalg.inv(self.orientation)\n",
    "            \n",
    "            # Transform points into the bbox's local coordinate system\n",
    "            transformed_xyz = (orientation_inv @ np.vstack((xyz.T, np.ones(xyz.shape[0])))).T\n",
    "            \n",
    "            # Get the half-dimensions of the bbox (since width is the full length)\n",
    "            half_width = self.width / 2\n",
    "            \n",
    "            # Check if points lie within the bbox in local coordinates\n",
    "            inside_bbox = (\n",
    "                (np.abs(transformed_xyz[:, 0]) <= half_width[0]) &\n",
    "                (np.abs(transformed_xyz[:, 1]) <= half_width[1]) &\n",
    "                (np.abs(transformed_xyz[:, 2]) <= half_width[2])\n",
    "            )\n",
    "            \n",
    "            # Return the indices of the points inside the bbox\n",
    "            return np.where(inside_bbox)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pointcloud import grid_subsample\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def grid_subsample_simple(xyz:np.ndarray, voxel_size:float, device='cuda'):\n",
    "    \"\"\"\n",
    "    Wrapper using numpy arrays\n",
    "    \"\"\"\n",
    "    sub = grid_subsample(torch.Tensor(xyz).to(device=device), voxel_size) # n -> m number of points\n",
    "    sub['points'] = sub['points'].to(dtype=torch.float64).detach().cpu().numpy()\n",
    "    sub['inv_inds'] = sub['inv_inds'].detach().cpu().numpy()\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02090091, 0.0100497 , 0.78400844],\n",
       "       [0.02898537, 0.06145073, 0.32086682],\n",
       "       [0.00134374, 0.04375704, 0.91919237],\n",
       "       ...,\n",
       "       [0.97715592, 0.96431643, 0.09565625],\n",
       "       [0.96305788, 0.98068589, 0.63602358],\n",
       "       [0.96210819, 0.96616167, 0.80586183]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((1000,3))\n",
    "sub = grid_subsample_simple(x, 0.04)\n",
    "sub['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial import KDTree\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jakteristics import compute_features\n",
    "\n",
    "import torch\n",
    "\n",
    "from utils.pointcloud import grid_subsample\n",
    "\n",
    "class FwfDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 proj_search_pattern,\n",
    "                 proj_query_list,\n",
    "                 return_fields = None,\n",
    "                 query_grid_size = None,\n",
    "                 subsample_on_gpu = True\n",
    "                 ):\n",
    "        \n",
    "        super(Dataset,self).__init__()\n",
    "        \n",
    "        self.proj_query_list = proj_query_list\n",
    "        self.return_fields = return_fields\n",
    "\n",
    "        self.projects = list()\n",
    "        \n",
    "        # keep track of point cloud sizes\n",
    "        self.proj_lens = []\n",
    "        \n",
    "        # go through search pattern\n",
    "        for fwf_prof_fp in glob(proj_search_pattern):\n",
    "            proj_name = os.path.basename(fwf_prof_fp)\n",
    "            \n",
    "            # only get projects if they are on the project list\n",
    "            if proj_name not in [p['proj_name'] for p in self.proj_query_list]:\n",
    "                continue\n",
    "            else:\n",
    "                bbox_queries = [p['bboxes'] for p in self.proj_query_list if p['proj_name']==proj_name][0]\n",
    "\n",
    "                \n",
    "            print(f\"Loading '{proj_name}'; Bounding box IDs = {bbox_queries if len(bbox_queries) else 'default'}\")\n",
    "\n",
    "            # load the data\n",
    "            pcd = pd.DataFrame(PlyData.read(list(glob(os.path.join(fwf_prof_fp,'labeled','*pointcloud.ply')))[0]).elements[0].data)\n",
    "            wfm = np.load(list(glob(os.path.join(fwf_prof_fp,'labeled','*waveform.npy')))[0])\n",
    "            meta = json.load(open(list(glob(os.path.join(fwf_prof_fp,'labeled','*metadata.json')))[0],\"r\"))\n",
    "\n",
    "\n",
    "            # get scan positions\n",
    "            if 'scanId=000' in meta['scan_positions'].keys():\n",
    "                sop = np.array([meta['scan_positions'][f'scanId={si:03}']['sop'] for si in range(len(meta['scan_positions']))])\n",
    "            else:\n",
    "                # handle case where only one scan position is in the metadata and it's f.s.r. labeled 'scanId=001' instead of 'scanID=000'\n",
    "                sop = np.array([meta['scan_positions']['scanId=001']['sop']])\n",
    "\n",
    "\n",
    "\n",
    "            # field names (constants)\n",
    "            riegl_feat_names = [ 'riegl_reflectance','riegl_amplitude', 'riegl_deviation', 'riegl_targetIndex','riegl_targetCount']\n",
    "            label_names = ['labels_0', 'labels_1', 'labels_2', 'labels_3']\n",
    "\n",
    "\n",
    "            # get full xyz\n",
    "            xyz_defaultBbox = pcd[['x','y','z']].to_numpy()\n",
    "            \n",
    "\n",
    "                \n",
    "            # save project as dict\n",
    "            if len(bbox_queries)==0:\n",
    "                sub = grid_subsample_simple(xyz_defaultBbox,query_grid_size, 'cuda' if subsample_on_gpu else 'cpu')\n",
    "                kd_tree = KDTree(xyz_defaultBbox)\n",
    "                _, sub_ids = kd_tree.query(sub['points'])\n",
    "                # handle default case\n",
    "                self.projects.append(dict(\n",
    "                    proj_name=f\"{proj_name}::defaultBbox\",\n",
    "                    xyz = xyz_defaultBbox,\n",
    "                    wfm = wfm,\n",
    "                    sop = sop,\n",
    "                    rgb = pcd[['Red','Green','Blue']].to_numpy(),\n",
    "                    riegl_feats = pcd[riegl_feat_names].to_numpy(),\n",
    "                    labels = pcd[label_names].to_numpy()[sub_ids], # labels need to be subsampeld \n",
    "                    sop_ids = pcd['scan_id'].to_numpy(),\n",
    "                    kd_tree = kd_tree,\n",
    "                    xyz_sub = sub['points'],\n",
    "                    sub_inv = sub['inv_inds']\n",
    "                ))\n",
    "                self.proj_lens.append(sub['points'].shape[0])\n",
    "            else:\n",
    "                # handle bboxes\n",
    "                for bbox_i in bbox_queries:\n",
    "                    bbox_meta = meta['bboxes'][f'bboxId={bbox_i:03}']\n",
    "                    bbox = BBox(orientation=np.array(bbox_meta['orientation']), width=np.array(bbox_meta['width']))\n",
    "                    subcloud_mask=bbox.cutout(xyz_defaultBbox)\n",
    "                    xyz_masked = xyz_defaultBbox[subcloud_mask]\n",
    "                    sub = grid_subsample_simple(xyz_masked,query_grid_size, 'cuda' if subsample_on_gpu else 'cpu')\n",
    "                    kd_tree = KDTree(xyz_masked)\n",
    "                    _, sub_ids = kd_tree.query(sub['points'])\n",
    "                    self.projects.append(dict(\n",
    "                        proj_name=f\"{proj_name}::bboxId={bbox_i:03}\",\n",
    "                        xyz = xyz_masked,\n",
    "                        wfm = wfm[subcloud_mask],\n",
    "                        sop = sop,\n",
    "                        rgb = pcd[['Red','Green','Blue']].to_numpy()[subcloud_mask],\n",
    "                        riegl_feats = pcd[riegl_feat_names].to_numpy()[subcloud_mask],\n",
    "                        labels = pcd[label_names].to_numpy()[subcloud_mask][sub_ids], # labels need to be subsampeld\n",
    "                        sop_ids = pcd['scan_id'].to_numpy()[subcloud_mask],\n",
    "                        kd_tree = KDTree(xyz_masked),\n",
    "                        xyz_sub = sub['points'],\n",
    "                        sub_inv = sub['inv_inds']\n",
    "                    ))\n",
    "                    self.proj_lens.append(sub['points'].shape[0])\n",
    "                    \n",
    "            # calculate the cumulative sum of the point cloud sizes\n",
    "\n",
    "            # break\n",
    "        self.proj_lens_cumsum = np.cumsum(self.proj_lens)\n",
    "    \n",
    "    def compute_neibors_knn(self, k:int):\n",
    "        for proj in self.projects:\n",
    "            print(f\"Computing neibors for '{proj['proj_name']}' @ k={k}\")\n",
    "            dists, neib_ids = proj['kd_tree'].query(proj['xyz_sub'], k)\n",
    "            proj['neibors'] = neib_ids\n",
    "    \n",
    "\n",
    "   \n",
    "    def compute_normals_knn(self):\n",
    "        for proj in self.projects:\n",
    "            k = proj['neibors'].shape[1]\n",
    "            print(f\"Computing normals for '{proj['proj_name']}' @ k={k}\")\n",
    "            neibs_xyz = proj['xyz'][proj['neibors']]\n",
    "\n",
    "            means = neibs_xyz.mean(axis=1, keepdims=True)\n",
    "            neibs_xyz -= means\n",
    "            cov = (neibs_xyz.transpose([0,2,1]) @ neibs_xyz) / (k-1)\n",
    "            eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "            # get non-flipped normals\n",
    "            normals = eigenvecs[:, :, 0]\n",
    "\n",
    "            # upsample normals to full resolution\n",
    "            normals = normals[proj['sub_inv']]\n",
    "            \n",
    "            # move all points to scanner CS\n",
    "            points_origin_scanPos = proj['sop'][proj['sop_ids']][:,:3,3]\n",
    "            xyz_scannerCs = proj['xyz'] - points_origin_scanPos\n",
    "            signs = np.sign(np.squeeze(xyz_scannerCs[:,None,:] @ normals [:,:,None])) * -1\n",
    "            normals *= signs[:,None]\n",
    "            \n",
    "            proj['normals'] = normals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    # def compute_normals_spherical_subsample(self, r = 0.08, voxel_size = 0.03, device='cuda'):\n",
    "        \n",
    "    #     for proj in self.projects:\n",
    "    #         print(f\"Computing normals for '{proj['proj_name']}' @ r={r:.3f}m and voxel_size = {voxel_size:.3f}\")\n",
    "    #         print(\"\\tDEBUG: subsampling\")\n",
    "    #         sub = grid_subsample(torch.Tensor(proj['xyz']).to(device=device), voxel_size) # n -> m number of points\n",
    "    #         sub['points'] = sub['points'].to(dtype=torch.float64).detach().cpu().numpy()\n",
    "    #         sub['inv_inds'] = sub['inv_inds'].detach().cpu().numpy()\n",
    "    #         _,sub_ids = proj['kd_tree'].query(sub['points'])\n",
    "    #         print(\"\\tDEBUG: computing feats\")\n",
    "    #         sub_normals = compute_features(sub['points'],search_radius=0.05,feature_names=['nx','ny','nz'])\n",
    "\n",
    "    #         # flip the normals\n",
    "    #         print(\"\\tDEBUG: flipping the subsampled normals\")\n",
    "    #         sub_xyz_scannerCs = proj['xyz'][sub_ids] - proj['sop'][proj['sop_ids']][:,3,:3][sub_ids]\n",
    "    #         sub_signs = np.sign(np.squeeze(sub_xyz_scannerCs[:,None,:] @ sub_normals[:,:,None]))*-1\n",
    "    #         sub_normals *= sub_signs[:,None]\n",
    "\n",
    "    #         print(\"\\tDEBUG: reprojecting\")\n",
    "    #         proj['normals'] = sub_normals[sub['inv_inds']]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # def compute_normals_spherical(self, r = 0.08):\n",
    "    #     for proj in self.projects:\n",
    "    #         print(f\"Computing normals for '{proj['proj_name']}' @ r={r:.3f}m\")\n",
    "            \n",
    "    #         # get non-oriented normals\n",
    "    #         normals = compute_features(proj['xyz'],search_radius=0.05,feature_names=['nx','ny','nz'])\n",
    "    #         # orient the normals to corresponding scanner position\n",
    "    #         xyz_scannerCs = proj['xyz'] - proj['sop'][proj['sop_ids']][:,3,:3]\n",
    "    #         signs = np.sign(np.squeeze(xyz_scannerCs[:,None,:] @ normals [:,:,None])) * -1\n",
    "    #         normals *= signs[:,None]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def compute_incAngles(self):\n",
    "        for proj in self.projects:\n",
    "            print(f\"Computing incidence angles for '{proj['proj_name']}'\")\n",
    "            xyz_scannerCs = proj['xyz'] - proj['sop'][proj['sop_ids']][:,:3,3]\n",
    "            proj['incAngles']= np.arccos(np.squeeze((proj['normals'][:,None,:] @ xyz_scannerCs[...,None])) / \\\n",
    "                (np.linalg.norm(xyz_scannerCs,axis=-1) * np.linalg.norm(proj['normals'],axis=-1)))\n",
    "            proj['distanceFromScanner'] = np.linalg.norm(xyz_scannerCs, axis=-1)\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get proj index first\n",
    "        proj_idx = np.argwhere(index<self.proj_lens_cumsum)[0][0]\n",
    "        size_prev = self.proj_lens_cumsum[proj_idx-1] if proj_idx > 0 else 0\n",
    "        residual_idx = index - size_prev\n",
    "        \n",
    "        # get neibors of point at index\n",
    "        neibs = self.projects[proj_idx]['neibors'][residual_idx]\n",
    "        \n",
    "        if self.return_fields == None:\n",
    "            # extract all fields\n",
    "            return dict(\n",
    "                xyz = self.projects[proj_idx]['xyz'][neibs],\n",
    "                wfm = self.projects[proj_idx]['wfm'][neibs],\n",
    "                rgb = self.projects[proj_idx]['rgb'][neibs],\n",
    "                riegl_feats = self.projects[proj_idx]['riegl_feats'][neibs],\n",
    "                labels = self.projects[proj_idx]['labels'][residual_idx],\n",
    "                normals = self.projects[proj_idx]['normals'][neibs],\n",
    "                incAngles = self.projects[proj_idx]['incAngles'][neibs],\n",
    "                distanceFromScanner = self.projects[proj_idx]['distanceFromScanner'][neibs],\n",
    "            )\n",
    "        else:\n",
    "            # only extract specific fields\n",
    "            return {f : self.projects[proj_idx][f][neibs] for f in self.return_fields}\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.sum([p['xyz_sub'].shape[0] for p in self.projects])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FGFeatNetwork (nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_input_feats,\n",
    "                 label_structure = {'labels_0':3, 'labels_1':10, 'labels_2':12, 'labels_3':18}\n",
    "                 ):\n",
    "        super(FGFeatNetwork, self).__init__()\n",
    "        \n",
    "        self.label_structure = label_structure\n",
    "        \n",
    "        # Pointwise feats\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(num_input_feats, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU()\n",
    "        )\n",
    "        self.wfm_conv = nn.Sequential(\n",
    "            # FIXME: check conv and maxpool kernel sizes and strides\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3), nn.RReLU(), nn.MaxPool1d(2), # activations 32 -> 16\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3), nn.RReLU(), nn.MaxPool1d(2),# activations 16 -> 8\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3), nn.RReLU(), nn.MaxPool1d(2),# activations 8 -> 4\n",
    "            # after concat. activation shape = 4 * 64 = 256\n",
    "        )\n",
    "        # TODO: Try out inverse bottleneck?\n",
    "        # TODO: Network size might be an overkill / unfeasable for the task\n",
    "        # MLP after concat with WFM feats\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(512, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 1024), nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.ModuleDict({k:nn.Linear(128,v) for k,v in self.label_structure.items()})\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Sequential(\n",
    "    nn.Linear(3,4), nn.ReLU(),\n",
    "    nn.Linear(4,2), nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (labels_0): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (labels_1): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (labels_2): Linear(in_features=128, out_features=12, bias=True)\n",
       "  (labels_3): Linear(in_features=128, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FGFeatNetwork(5)\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '2024-03-22_FW_Koenigshuegel.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_02.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_03.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_04.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_05.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-05-10_FW_RWTH_Zentrum_01.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-07-31_FW_Bruecke_Koenigstr.FwfProj'; Bounding box IDs = [0, 2]\n",
      "Loading '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Deltourserb.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj'; Bounding box IDs = [1]\n",
      "Loading '2024-08-02_FW_Bruecke_RotheErde.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj'; Bounding box IDs = default\n"
     ]
    }
   ],
   "source": [
    "# data_dir = r\"D:\\Projekte\\GIA_220412_PCS\\02_Datasets\\FullWaveForm\\full_waveform_software\\FullWaveformParse_mass\\output\\FWF_Aachen_labeled\\*.FwfProj\"              \n",
    "proj_search_pattern = \"../../02_Datasets/FWF_Aachen_labeled/*.FwfProj\"      \n",
    "train_ds = FwfDataset(proj_search_pattern=proj_search_pattern, proj_query_list=train_projects, query_grid_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing neibors for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=20\n",
      "Computing incidence angles for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox'\n"
     ]
    }
   ],
   "source": [
    "train_ds.compute_neibors_knn(k=20)\n",
    "train_ds.compute_normals_knn()\n",
    "train_ds.compute_incAngles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304701"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['xyz'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32)\n",
    "\n",
    "i = 0\n",
    "for batch in train_dl:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    if i > 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize normals\n",
    "# proj = train_ds.projects[7]\n",
    "for i, proj in enumerate(train_ds.projects):\n",
    "    PlyData([PlyElement.describe(pd.DataFrame(np.concatenate([proj['xyz'],proj['normals'],np.squeeze(proj['incAngles'])[:,None],proj['distanceFromScanner'][:,None], proj['sop_ids'][:,None]], axis=1),columns=['x','y','z','nx','ny','nz','incAngle','distanceFromScanner','sop_ids']).to_records(index=False),'vertex')]).write(f\"./_temp/proj_{i:03}.ply\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
