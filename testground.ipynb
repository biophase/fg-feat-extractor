{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projects = [ # total: 13 point clouds (including bbox splitting)\n",
    "    # {\"proj_name\": \"2023-08-28_FW_EingangBauing.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-03-22_FW_Koenigshuegel.FwfProj\", \"bboxes\" : []},                      # building\n",
    "    # {\"proj_name\": \"2024-04-05_FW_Westbahnhof_01.FwfProj\", \"bboxes\" : []},                   # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_02.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_03.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_04.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_05.FwfProj\", \"bboxes\" : []},                     # building\n",
    "    {\"proj_name\": \"2024-05-10_FW_RWTH_Zentrum_01.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Koenigstr.FwfProj\", \"bboxes\" : [0,2]},               # bridge (steel+stone)\n",
    "    # {\"proj_name\": \"2024-07-31_FW_Bruecke_Turmstr.FwfProj\", \"bboxes\" : []},                  # bridge (concrete/steel)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj\", \"bboxes\" : []},   # bridge (concrete)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Deltourserb.FwfProj\", \"bboxes\" : []},                # bridge (concrete)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj\", \"bboxes\" : [1]},             # bridge (concrete, steel, brick)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_RotheErde.FwfProj\", \"bboxes\" : []},                  # bridge (steel)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Rottstrasse.FwfProj\", \"bboxes\" : []},                # bridge (concrete)\n",
    "]\n",
    "val_projects = [ # total: 5 point clouds (including bbox splitting)\n",
    "    {\"proj_name\": \"2023-08-28_FW_EingangBauing.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_01.FwfProj\", \"bboxes\" : []},                   # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Koenigstr.FwfProj\", \"bboxes\" : [1]},               # bridge (steel+stone)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj\", \"bboxes\" : [0]},           # bridge (concrete, steel, brick)\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Turmstr.FwfProj\", \"bboxes\" : []},                  # bridge (concrete/steel)\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BBox:\n",
    "    \"\"\"Bounding box using the CloudCompare synthax\n",
    "    \"\"\"\n",
    "    def __init__(self, orientation:np.ndarray, width: np.ndarray) -> None:\n",
    "        # an easy way to retrieve these values is by using the cross-section tool in cloudcompare\n",
    "        # so cross-section -> advanced -> orientation -> to clipboard\n",
    "        self.orientation = orientation\n",
    "        # from the same menu copy and paste the values under 'width' \n",
    "        self.width = width\n",
    "        \n",
    "    def cutout(self, xyz:np.ndarray) -> np.ndarray:\n",
    "            # Inverse of the orientation matrix (rotation and translation)\n",
    "            orientation_inv = np.linalg.inv(self.orientation)\n",
    "            \n",
    "            # Transform points into the bbox's local coordinate system\n",
    "            transformed_xyz = (orientation_inv @ np.vstack((xyz.T, np.ones(xyz.shape[0])))).T\n",
    "            \n",
    "            # Get the half-dimensions of the bbox (since width is the full length)\n",
    "            half_width = self.width / 2\n",
    "            \n",
    "            # Check if points lie within the bbox in local coordinates\n",
    "            inside_bbox = (\n",
    "                (np.abs(transformed_xyz[:, 0]) <= half_width[0]) &\n",
    "                (np.abs(transformed_xyz[:, 1]) <= half_width[1]) &\n",
    "                (np.abs(transformed_xyz[:, 2]) <= half_width[2])\n",
    "            )\n",
    "            \n",
    "            # Return the indices of the points inside the bbox\n",
    "            return np.where(inside_bbox)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pointcloud import grid_subsample\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def grid_subsample_simple(xyz:np.ndarray, voxel_size:float, device='cuda'):\n",
    "    \"\"\"\n",
    "    Wrapper using numpy arrays\n",
    "    \"\"\"\n",
    "    sub = grid_subsample(torch.Tensor(xyz).to(device=device), voxel_size) # n -> m number of points\n",
    "    sub['points'] = sub['points'].to(dtype=torch.float64).detach().cpu().numpy()\n",
    "    sub['inv_inds'] = sub['inv_inds'].detach().cpu().numpy()\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple, List\n",
    "import colorsys\n",
    "\n",
    "class Transform():\n",
    "    def __init__(self, fn:Callable):\n",
    "        self.fn = fn\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "\n",
    "class TransZRotation(Transform):\n",
    "    def __init__(self, limits:Tuple = (0, 2*np.pi)):\n",
    "        def fn(x):\n",
    "            r = np.random.uniform(*limits)\n",
    "            rot_matrix = np.array([[np.cos(r), -np.sin(r), 0],\n",
    "                                   [np.sin(r),  np.cos(r), 0],\n",
    "                                   [0,          0,         1]])\n",
    "            return x @ rot_matrix\n",
    "            \n",
    "        super().__init__(fn)\n",
    "\n",
    "\n",
    "class TransScaling(Transform):\n",
    "    def __init__(self, limits: Tuple = (0.85, 1.15)):\n",
    "        def fn(x):\n",
    "            scale_factor = np.random.uniform(*limits)\n",
    "            scale_matrix = np.diag([scale_factor, scale_factor, scale_factor])  # Uniform scaling\n",
    "            return x @ scale_matrix\n",
    "        \n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransGaussianNoise(Transform):\n",
    "    def __init__(self, mean: float = 0.0, std: float = 0.01):\n",
    "        def fn(x:np.ndarray):\n",
    "            noise = np.random.normal(loc=mean, scale=std, size=x.shape)\n",
    "            return x + noise\n",
    "\n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransGammaCorrection(Transform):\n",
    "    def __init__(self, limits: Tuple[float, float] = (0.8, 1.2)):\n",
    "        def fn(x):\n",
    "            gamma = np.random.uniform(*limits)\n",
    "            return np.clip(x ** gamma, 0, 1) # TODO: Check order of normalization and augmentation !!!\n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransSignalScaling(Transform):\n",
    "    def __init__(self,limits:Tuple[float,float] = (0.95,1.05)):\n",
    "        def fn(x:np.ndarray):\n",
    "            # waveform shape is neibors, 32\n",
    "            scale_factors = np.random.uniform(*limits, size=(x.shape[0],1))\n",
    "            return x * scale_factors\n",
    "        \n",
    "        super().__init__(fn)\n",
    "        \n",
    "class TransFeatureDropout(Transform):\n",
    "    def __init__(self, dropout_prob: float = 0.2):\n",
    "\n",
    "        def fn(x):\n",
    "            mask = np.random.binomial(1, 1 - dropout_prob, size=x.shape[1])\n",
    "            return x * mask  \n",
    "        \n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransStandardize(Transform):\n",
    "    def __init__(self, mean, std):\n",
    "        def fn(x:np.ndarray):\n",
    "            x -= mean\n",
    "            x /= std\n",
    "            return x\n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransformsList():\n",
    "    def __init__(self,\n",
    "                 transforms:List[Transform]):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_idx = 56\n",
    "# proj = train_ds.projects[0]\n",
    "# neibs = proj['neibors'][res_idx]\n",
    "# rgb = proj['rgb'][neibs]\n",
    "\n",
    "\n",
    "# transform = TransformsList([\n",
    "#     #  TransGammaCorrection(),\n",
    "#     TransStandardize(mean=np.array([[0.29689665, 0.3428666,  0.190237]]),std=np.array([[0.21558372, 0.23351644, 0.21213871]])),\n",
    "#     TransGaussianNoise(0,0.02),\n",
    "#     TransFeatureDropout(dropout_prob=0.1)                         \n",
    "# ])\n",
    "\n",
    "# print(rgb.shape, rgb.mean(), rgb.std())\n",
    "# rgb = transform(rgb)\n",
    "# print(rgb.shape, rgb.mean(), rgb.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial import KDTree\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jakteristics import compute_features\n",
    "from typing import List, Optional, Dict\n",
    "import torch\n",
    "\n",
    "from utils.pointcloud import grid_subsample\n",
    "\n",
    "class FwfDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 proj_search_pattern,\n",
    "                 proj_query_list,\n",
    "                 return_fields_input = [\n",
    "                    'xyz',\n",
    "                    'rgb',\n",
    "                    'riegl_feats',\n",
    "                    'normals',\n",
    "                    'incAngles',\n",
    "                    'distanceFromScanner',\n",
    "                ],\n",
    "                 label_names = None,\n",
    "                 return_waveform = True,\n",
    "                 query_grid_size = None,\n",
    "                 subsample_on_gpu = True,\n",
    "                 transforms_dict: Dict[str, TransformsList] = dict(\n",
    "                     xyz = TransformsList([\n",
    "                         TransZRotation(),\n",
    "                         TransScaling(),\n",
    "                         TransGaussianNoise(0,0.002)\n",
    "                     ]),\n",
    "                     rgb = TransformsList([\n",
    "                        #  TransGammaCorrection(),\n",
    "                        #  TransStandardize(mean=np.array([[0.29689665, 0.3428666,  0.190237]]),std=np.array([[0.21558372, 0.23351644, 0.21213871]])),\n",
    "                         TransGaussianNoise(0,0.02),\n",
    "                         TransFeatureDropout(dropout_prob=0.1)                         \n",
    "                     ]),\n",
    "                     wfm = TransformsList([\n",
    "                        # TransStandardize(mean=358.35934, std=623.0141),\n",
    "                        TransSignalScaling(),\n",
    "                     ]),\n",
    "                     normals = TransformsList([\n",
    "                         TransGaussianNoise(0,0.05)\n",
    "                     ]),\n",
    "                     incAngles = TransformsList([\n",
    "                         TransGaussianNoise(0,0.05),\n",
    "                         TransStandardize(mean = 2.07, std = 0.38),\n",
    "                     ]),\n",
    "                     distanceFromScanner = TransformsList([\n",
    "                         TransGaussianNoise(0,0.1),\n",
    "                         TransStandardize(mean = 12.13, std = 12.20),\n",
    "                     ]),\n",
    "                    #  riegl_feats = TransformsList([\n",
    "                    #      TransStandardize(mean=np.array([[-6.90191949, 25.16398933, 26.45952891,  1.,  1.03636612]]), std= np.array([[2.32590898, 2.98518547, 929.71399545, 1., 0.22651793]]))\n",
    "                    #  ])\n",
    "                 )\n",
    "                 ):\n",
    "        \n",
    "        super(Dataset,self).__init__()\n",
    "        \n",
    "        self.proj_query_list = proj_query_list\n",
    "        self.return_fields_input = return_fields_input\n",
    "        self.return_waveform = return_waveform\n",
    "        self.transforms_dict = transforms_dict\n",
    "\n",
    "        self.projects = list()\n",
    "        \n",
    "        # keep track of point cloud sizes\n",
    "        self.proj_lens = []\n",
    "\n",
    "        \n",
    "        counter = 0\n",
    "        # go through search pattern\n",
    "        for fwf_prof_fp in glob(proj_search_pattern):\n",
    "            proj_name = os.path.basename(fwf_prof_fp)\n",
    "            \n",
    "            # only get projects if they are on the project list\n",
    "            if proj_name not in [p['proj_name'] for p in self.proj_query_list]:\n",
    "                continue\n",
    "            else:\n",
    "                bbox_queries = [p['bboxes'] for p in self.proj_query_list if p['proj_name']==proj_name][0]\n",
    "\n",
    "                \n",
    "            print(f\"Loading '{proj_name}'; Bounding box IDs = {bbox_queries if len(bbox_queries) else 'default'}\")\n",
    "\n",
    "            # load the data\n",
    "            pcd = pd.DataFrame(PlyData.read(list(glob(os.path.join(fwf_prof_fp,'labeled','*pointcloud.ply')))[0]).elements[0].data)\n",
    "            wfm = np.load(list(glob(os.path.join(fwf_prof_fp,'labeled','*waveform.npy')))[0]).astype(np.float32)\n",
    "            meta = json.load(open(list(glob(os.path.join(fwf_prof_fp,'labeled','*metadata.json')))[0],\"r\"))\n",
    "            \n",
    "            # field names (constants)\n",
    "            riegl_feat_names = [ 'riegl_reflectance','riegl_amplitude', 'riegl_deviation', 'riegl_targetIndex','riegl_targetCount']\n",
    "            # label_names = ['labels_0', 'labels_1', 'labels_2', 'labels_3']\n",
    "            self.label_names:Optional[List[str]] = label_names\n",
    "            \n",
    "            \n",
    "            # convert to numpy\n",
    "            riegl_feats = pcd[riegl_feat_names].to_numpy()\n",
    "            rgb = pcd[['Red','Green','Blue']].to_numpy()\n",
    "            \n",
    "            # normalize the data\n",
    "            # FIXME: Statistics calculated on first point cloud only\n",
    "            wfm = (wfm.astype(np.float32) - 358.35934) / 623.0141 # - mean / std\n",
    "\n",
    "            riegl_feats -= np.array([[-6.90191949, 25.16398933, 26.45952891,  1.        ,  1.03636612]]) # - means\n",
    "            riegl_feats /= np.array([[  2.32590898, 2.98518547, 929.71399545, 1., 0.22651793]]) # /std\n",
    "            \n",
    "            rgb -= np.array([[0.29689665, 0.3428666,  0.190237]]) # means\n",
    "            rgb /= np.array([[0.21558372, 0.23351644, 0.21213871]]) # std          \n",
    "            \n",
    "\n",
    "            \n",
    "            # get scan positions\n",
    "            if 'scanId=000' in meta['scan_positions'].keys():\n",
    "                sop = np.array([meta['scan_positions'][f'scanId={si:03}']['sop'] for si in range(len(meta['scan_positions']))])\n",
    "            else:\n",
    "                # handle case where only one scan position is in the metadata and it's f.s.r. labeled 'scanId=001' instead of 'scanID=000'\n",
    "                sop = np.array([meta['scan_positions']['scanId=001']['sop']])\n",
    "\n",
    "            # get full xyz\n",
    "            xyz_defaultBbox = pcd[['x','y','z']].to_numpy()       \n",
    "\n",
    "                \n",
    "            # save project as dict\n",
    "            if len(bbox_queries)==0:\n",
    "                # handle default case\n",
    "                sub = grid_subsample_simple(xyz_defaultBbox,query_grid_size, 'cuda' if subsample_on_gpu else 'cpu')\n",
    "                kd_tree = KDTree(xyz_defaultBbox)\n",
    "                _, sub_ids = kd_tree.query(sub['points'])\n",
    "                self.projects.append(dict(\n",
    "                    proj_name=f\"{proj_name}::defaultBbox\",\n",
    "                    xyz = xyz_defaultBbox,\n",
    "                    wfm = wfm,\n",
    "                    sop = sop,\n",
    "                    rgb = rgb,\n",
    "                    riegl_feats = riegl_feats,\n",
    "                    labels = pcd[self.label_names].to_numpy()[sub_ids], # labels need to be subsampeld \n",
    "                    sop_ids = pcd['scan_id'].to_numpy(),\n",
    "                    kd_tree = kd_tree,\n",
    "                    xyz_sub = sub['points'],\n",
    "                    sub_inv = sub['inv_inds']\n",
    "                ))\n",
    "                self.proj_lens.append(sub['points'].shape[0])\n",
    "            else:\n",
    "                # handle region bboxes case\n",
    "                for bbox_i in bbox_queries:\n",
    "                    bbox_meta = meta['bboxes'][f'bboxId={bbox_i:03}']\n",
    "                    bbox = BBox(orientation=np.array(bbox_meta['orientation']), width=np.array(bbox_meta['width']))\n",
    "                    subcloud_mask=bbox.cutout(xyz_defaultBbox)\n",
    "                    xyz_masked = xyz_defaultBbox[subcloud_mask]\n",
    "                    sub = grid_subsample_simple(xyz_masked,query_grid_size, 'cuda' if subsample_on_gpu else 'cpu')\n",
    "                    kd_tree = KDTree(xyz_masked)\n",
    "                    _, sub_ids = kd_tree.query(sub['points'])\n",
    "                    self.projects.append(dict(\n",
    "                        proj_name=f\"{proj_name}::bboxId={bbox_i:03}\",\n",
    "                        xyz = xyz_masked,\n",
    "                        wfm = wfm[subcloud_mask],\n",
    "                        sop = sop,\n",
    "                        rgb = rgb[subcloud_mask],\n",
    "                        riegl_feats = riegl_feats[subcloud_mask],\n",
    "                        labels = pcd[self.label_names].to_numpy()[subcloud_mask][sub_ids], # labels need to be subsampeld\n",
    "                        sop_ids = pcd['scan_id'].to_numpy()[subcloud_mask],\n",
    "                        kd_tree = KDTree(xyz_masked),\n",
    "                        xyz_sub = sub['points'],\n",
    "                        sub_inv = sub['inv_inds']\n",
    "                    ))\n",
    "                    self.proj_lens.append(sub['points'].shape[0])\n",
    "                    \n",
    "            # calculate the cumulative sum of the point cloud sizes\n",
    "\n",
    "            if counter >= 3:\n",
    "                continue\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "        self.proj_lens_cumsum = np.cumsum(self.proj_lens)\n",
    "    \n",
    "    def compute_neibors_knn(self, k:int):\n",
    "        for proj in self.projects:\n",
    "            print(f\"Computing neibors for '{proj['proj_name']}' @ k={k}\")\n",
    "            dists, neib_ids = proj['kd_tree'].query(proj['xyz_sub'], k)\n",
    "            proj['neibors'] = neib_ids\n",
    "    \n",
    "\n",
    "   \n",
    "    def compute_normals_knn(self):\n",
    "        for proj in self.projects:\n",
    "            k = proj['neibors'].shape[1]\n",
    "            print(f\"Computing normals for '{proj['proj_name']}' @ k={k}\")\n",
    "            neibs_xyz = proj['xyz'][proj['neibors']]\n",
    "\n",
    "            means = neibs_xyz.mean(axis=1, keepdims=True)\n",
    "            neibs_xyz -= means\n",
    "            cov = (neibs_xyz.transpose([0,2,1]) @ neibs_xyz) / (k-1)\n",
    "            eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "            # get non-flipped normals\n",
    "            normals = eigenvecs[:, :, 0]\n",
    "\n",
    "            # upsample normals to full resolution\n",
    "            normals = normals[proj['sub_inv']]\n",
    "            \n",
    "            # move all points to scanner CS\n",
    "            points_origin_scanPos = proj['sop'][proj['sop_ids']][:,:3,3]\n",
    "            xyz_scannerCs = proj['xyz'] - points_origin_scanPos\n",
    "            signs = np.sign(np.squeeze(xyz_scannerCs[:,None,:] @ normals [:,:,None])) * -1\n",
    "            normals *= signs[:,None]\n",
    "            \n",
    "            proj['normals'] = normals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    # def compute_normals_spherical_subsample(self, r = 0.08, voxel_size = 0.03, device='cuda'):\n",
    "        \n",
    "    #     for proj in self.projects:\n",
    "    #         print(f\"Computing normals for '{proj['proj_name']}' @ r={r:.3f}m and voxel_size = {voxel_size:.3f}\")\n",
    "    #         print(\"\\tDEBUG: subsampling\")\n",
    "    #         sub = grid_subsample(torch.Tensor(proj['xyz']).to(device=device), voxel_size) # n -> m number of points\n",
    "    #         sub['points'] = sub['points'].to(dtype=torch.float64).detach().cpu().numpy()\n",
    "    #         sub['inv_inds'] = sub['inv_inds'].detach().cpu().numpy()\n",
    "    #         _,sub_ids = proj['kd_tree'].query(sub['points'])\n",
    "    #         print(\"\\tDEBUG: computing feats\")\n",
    "    #         sub_normals = compute_features(sub['points'],search_radius=0.05,feature_names=['nx','ny','nz'])\n",
    "\n",
    "    #         # flip the normals\n",
    "    #         print(\"\\tDEBUG: flipping the subsampled normals\")\n",
    "    #         sub_xyz_scannerCs = proj['xyz'][sub_ids] - proj['sop'][proj['sop_ids']][:,3,:3][sub_ids]\n",
    "    #         sub_signs = np.sign(np.squeeze(sub_xyz_scannerCs[:,None,:] @ sub_normals[:,:,None]))*-1\n",
    "    #         sub_normals *= sub_signs[:,None]\n",
    "\n",
    "    #         print(\"\\tDEBUG: reprojecting\")\n",
    "    #         proj['normals'] = sub_normals[sub['inv_inds']]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # def compute_normals_spherical(self, r = 0.08):\n",
    "    #     for proj in self.projects:\n",
    "    #         print(f\"Computing normals for '{proj['proj_name']}' @ r={r:.3f}m\")\n",
    "            \n",
    "    #         # get non-oriented normals\n",
    "    #         normals = compute_features(proj['xyz'],search_radius=0.05,feature_names=['nx','ny','nz'])\n",
    "    #         # orient the normals to corresponding scanner position\n",
    "    #         xyz_scannerCs = proj['xyz'] - proj['sop'][proj['sop_ids']][:,3,:3]\n",
    "    #         signs = np.sign(np.squeeze(xyz_scannerCs[:,None,:] @ normals [:,:,None])) * -1\n",
    "    #         normals *= signs[:,None]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def compute_incAngles(self):\n",
    "        for proj in self.projects:\n",
    "            print(f\"Computing incidence angles for '{proj['proj_name']}'\")\n",
    "            xyz_scannerCs = proj['xyz'] - proj['sop'][proj['sop_ids']][:,:3,3]\n",
    "            proj['incAngles']= np.arccos(np.squeeze((proj['normals'][:,None,:] @ xyz_scannerCs[...,None])) / \\\n",
    "                (np.linalg.norm(xyz_scannerCs,axis=-1) * np.linalg.norm(proj['normals'],axis=-1)))\n",
    "            proj['distanceFromScanner'] = np.linalg.norm(xyz_scannerCs, axis=-1)\n",
    "            \n",
    "            # reshape by adding an additional axis\n",
    "            proj['incAngles'] = proj['incAngles'][:,None]\n",
    "            proj['distanceFromScanner'] = proj['distanceFromScanner'][:,None]\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get proj index first\n",
    "        proj_idx = np.argwhere(index<self.proj_lens_cumsum)[0][0]\n",
    "        size_prev = self.proj_lens_cumsum[proj_idx-1] if proj_idx > 0 else 0\n",
    "        residual_idx = index - size_prev\n",
    "        \n",
    "        # get neibors of point at index\n",
    "        neibs = self.projects[proj_idx]['neibors'][residual_idx]\n",
    "        \n",
    "        \n",
    "\n",
    "        # TODO: remove the default option (instead putting the fields as a default argument list to constructor)\n",
    "\n",
    "            # features_neibors = np.concatenate([\n",
    "            #     self.projects[proj_idx]['xyz'][neibs],\n",
    "            #     self.projects[proj_idx]['rgb'][neibs],\n",
    "            #     self.projects[proj_idx]['riegl_feats'][neibs],\n",
    "            #     self.projects[proj_idx]['normals'][neibs],\n",
    "            #     self.projects[proj_idx]['incAngles'][neibs],\n",
    "            #     self.projects[proj_idx]['distanceFromScanner'][neibs],\n",
    "            # ], axis=-1).astype(np.float32),\n",
    "            # features_point = np.concatenate([\n",
    "            #     self.projects[proj_idx]['xyz'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['rgb'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['riegl_feats'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['normals'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['incAngles'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['distanceFromScanner'][residual_idx][None,:],\n",
    "            # ],axis=-1).astype(np.float32)\n",
    "\n",
    "\n",
    "        return_dict = dict(\n",
    "            features_neibors = np.concatenate(\n",
    "                [\n",
    "                    self.transforms_dict[f](self.projects[proj_idx][f][neibs])\n",
    "                    if f in self.transforms_dict else self.projects[proj_idx][f][neibs]\n",
    "                    for f in self.return_fields_input\n",
    "                ],\n",
    "                axis=-1\n",
    "            ).astype(np.float32),\n",
    "            features_point = np.concatenate(\n",
    "                [\n",
    "                    self.transforms_dict[f](self.projects[proj_idx][f][residual_idx][None,:])\n",
    "                    if f in self.transforms_dict else self.projects[proj_idx][f][residual_idx][None,:]\n",
    "                    for f in self.return_fields_input\n",
    "                ],\n",
    "                axis=-1\n",
    "            ).astype(np.float32)\n",
    "        )\n",
    "\n",
    "        # add waveforms and labels\n",
    "        return_dict.update(dict(\n",
    "            wfm_neibors = self.projects[proj_idx]['wfm'][neibs] if self.return_waveform else None,\n",
    "            wfm_point = self.projects[proj_idx]['wfm'][residual_idx][None,:] if self.return_waveform else None,\n",
    "            labels = self.projects[proj_idx]['labels'][residual_idx],\n",
    "        )) # type:ignore\n",
    "        \n",
    "\n",
    "        \n",
    "        return return_dict\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.sum([p['xyz_sub'].shape[0] for p in self.projects])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FGFeatNetwork (nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_input_feats,\n",
    "                 label_structure = {'labels_0':3, 'labels_1':10, 'labels_2':12, 'labels_3':18},\n",
    "                 dropout_prob = 0.2,\n",
    "                 global_constraint = False,\n",
    "                 ):\n",
    "        super(FGFeatNetwork, self).__init__()\n",
    "        \n",
    "        self.label_structure = label_structure\n",
    "        self.global_constraint = global_constraint\n",
    "        # Pointwise feats\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(num_input_feats, 64), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 128), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(128, 128), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "        self.wf_conv = nn.Sequential(\n",
    "            # FIXME: check conv and maxpool kernel sizes and strides\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3), nn.RReLU(),nn.BatchNorm1d(16), nn.MaxPool1d(2), # activations 32 -> 16\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3), nn.RReLU(),nn.BatchNorm1d(32), nn.MaxPool1d(2),# activations 16 -> 8\n",
    "            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3), nn.RReLU(),nn.BatchNorm1d(32), nn.MaxPool1d(2),# activations 8 -> 4\n",
    "            # after concat. activation shape = 4 * 64 = 256\n",
    "        )\n",
    "        # TODO: Try out inverse bottleneck?\n",
    "        # TODO: Network size might be an overkill / unfeasable for the task\n",
    "        \n",
    "        # MLP after concat with WFM feats\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(192, 512), nn.ReLU(), nn.Dropout(p=dropout_prob), \n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(704, 512), nn.ReLU(), nn.Dropout(p=dropout_prob),  # joined shape (point + neibors)\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.ModuleDict({k:nn.Linear(128,v) for k,v in self.label_structure.items()})\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # handle neibor features\n",
    "        pw_feats_neib = self.mlp1(x['features_neibors'])\n",
    "        batch, neibor, signal_length = x['wfm_neibors'].shape\n",
    "        wf_feats_neib = self.wf_conv(x['wfm_neibors'].view((batch * neibor, 1, signal_length))) # batch, neibor, signal_length -> batch * neibor, 1,  signal_length\n",
    "        wf_feats_neib = wf_feats_neib.view((batch, neibor,-1))\n",
    "        joined_feats_neib = torch.cat([pw_feats_neib, wf_feats_neib], dim=-1)\n",
    "        \n",
    "        # handle the point features\n",
    "        pw_feats_point = self.mlp1(x['features_point'])\n",
    "        batch, neibor, signal_length = x['wfm_point'].shape\n",
    "        wf_feats_point = self.wf_conv(x['wfm_point'].view((batch * neibor, 1, signal_length))) ## batch, 1, signal_length -> batch , 1,  signal_length\n",
    "        wf_feats_point = wf_feats_point.view((batch, neibor,-1))\n",
    "        joined_feats_point = torch.cat([pw_feats_point, wf_feats_point], dim=-1)\n",
    "        \n",
    "        # continue processing neibor feats\n",
    "        \n",
    "        joined_feats_neib = self.mlp2(joined_feats_neib)\n",
    "        joined_feats_neib = torch.max(joined_feats_neib, dim=1)[0]\n",
    "        \n",
    "        # join neibor with skip connection to point features\n",
    "        global_feat = torch.cat([joined_feats_neib, torch.squeeze(joined_feats_point)],dim=-1)\n",
    "        if self.global_constraint:\n",
    "            global_feat = global_feat / (torch.norm(global_feat, p=2, dim=1, keepdim=True)+1e-6)\n",
    "\n",
    "        global_feat = self.mlp3(global_feat)\n",
    "        \n",
    "        result = {k: self.classifier[k](global_feat) for k in self.classifier.keys()}\n",
    "        \n",
    "        \n",
    "        \n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '2024-03-22_FW_Koenigshuegel.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_02.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_03.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_04.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_05.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-05-10_FW_RWTH_Zentrum_01.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-07-31_FW_Bruecke_Koenigstr.FwfProj'; Bounding box IDs = [0, 2]\n",
      "Loading '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Deltourserb.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj'; Bounding box IDs = [1]\n",
      "Loading '2024-08-02_FW_Bruecke_RotheErde.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj'; Bounding box IDs = default\n",
      "Loading '2023-08-28_FW_EingangBauing.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_01.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-07-31_FW_Bruecke_Koenigstr.FwfProj'; Bounding box IDs = [1]\n",
      "Loading '2024-07-31_FW_Bruecke_Turmstr.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj'; Bounding box IDs = [0]\n",
      "Computing neibors for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=20\n",
      "Computing incidence angles for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox'\n",
      "Computing neibors for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000' @ k=20\n",
      "Computing normals for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000' @ k=20\n",
      "Computing incidence angles for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000'\n",
      "Computing neibors for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000' @ k=96\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "device='cuda'\n",
    "label_names = [\n",
    "    'labels_0',\n",
    "    # 'labels_1',\n",
    "    # 'labels_2',\n",
    "    # 'labels_3',\n",
    "]\n",
    "query_grid_size = 0.1\n",
    "num_neib_normals = 20\n",
    "num_neib_featextraction = 96\n",
    "\n",
    "\n",
    "\n",
    "# create Dataset\n",
    "proj_search_pattern = r\"D:\\Projekte\\GIA_220412_PCS\\02_Datasets\\FullWaveForm\\full_waveform_software\\FullWaveformParse_mass\\output\\FWF_Aachen_labeled\\*.FwfProj\" # work pc\n",
    "# proj_search_pattern = \"../../02_Datasets/FWF_Aachen_labeled/*.FwfProj\" # home pc\n",
    "train_ds = FwfDataset(\n",
    "    proj_search_pattern=proj_search_pattern, \n",
    "    proj_query_list=train_projects, \n",
    "    query_grid_size=query_grid_size,\n",
    "    label_names = label_names,\n",
    "    return_waveform = True,\n",
    "    subsample_on_gpu = True\n",
    ")\n",
    "val_ds = FwfDataset(\n",
    "    proj_search_pattern=proj_search_pattern, \n",
    "    proj_query_list=val_projects, \n",
    "    query_grid_size=query_grid_size,\n",
    "    label_names = label_names,\n",
    "    return_waveform = True,\n",
    "    subsample_on_gpu = True\n",
    ")\n",
    "\n",
    "# rest computation\n",
    "for ds in [train_ds, val_ds]:\n",
    "    ds.compute_neibors_knn(k=num_neib_normals)\n",
    "    ds.compute_normals_knn()\n",
    "    ds.compute_incAngles()\n",
    "    ds.compute_neibors_knn(k=num_neib_featextraction)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = FGFeatNetwork(\n",
    "    num_input_feats = train_ds[0]['features_neibors'].shape[-1],\n",
    "    label_structure = {\n",
    "        \"labels_0\":3,\n",
    "        # \"labels_1\":8,\n",
    "        # \"labels_2\":18,\n",
    "        # \"labels_3\":18\n",
    "    },\n",
    "    ).to(device=device)\n",
    "\n",
    "# create dataloader\n",
    "\n",
    "batch_size=1024\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=12, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, num_workers=12, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    if i % 10:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'int' and 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mFwfDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# get proj index first\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m     proj_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margwhere(\u001b[43mindex\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_lens_cumsum\u001b[49m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    275\u001b[0m     size_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_lens_cumsum[proj_idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m proj_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    276\u001b[0m     residual_idx \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m-\u001b[39m size_prev\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'slice'"
     ]
    }
   ],
   "source": [
    "DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training       :   0%|          | 0/3607 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'TransZRotation.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dl, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[0;32m     16\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# put batch on device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1032\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'TransZRotation.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "weight_decay = 1e-4\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "optim = Adam(params=model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "\n",
    "    epoch_train_loss = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_dl, desc=f\"{'Training':<15}\", leave=True)):\n",
    "        optim.zero_grad()\n",
    "        # put batch on device\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device=device)\n",
    "\n",
    "        # forward pass\n",
    "        result = model(batch)\n",
    "        \n",
    "        # agregate loss on all levels\n",
    "        loss = torch.tensor(0.).to(device=device)\n",
    "        for k in result.keys():\n",
    "            output = result[k]\n",
    "            gt = batch['labels'][:, train_ds.label_names.index(k)] # type:ignore\n",
    "            loss += criterion(output, gt)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    # validate    \n",
    "    epoch_val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(tqdm(val_dl, desc=f\"{'Validation':<15}\", leave=True)):\n",
    "            # put batch on device\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device=device)\n",
    "\n",
    "            # forward pass\n",
    "            result = model(batch)\n",
    "            \n",
    "            # agregate loss on all levels\n",
    "            loss = torch.tensor(0.).to(device=device)\n",
    "            for k in result.keys():\n",
    "                output = result[k]\n",
    "                gt = batch['labels'][:, val_ds.label_names.index(k)] # type:ignore\n",
    "                loss += criterion(output, gt)\n",
    "            \n",
    "            epoch_val_loss.append(loss.item())\n",
    "    print(f\"Epoch-{epoch:03}: Lt:{np.mean(epoch_train_loss):.4f} Lv:{np.mean(epoch_val_loss):.4f}\")\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training       : 100%|██████████| 1804/1804 [19:05<00:00,  1.57it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [07:57<00:00,  1.75it/s]\n",
    "# Epoch-000: Lt:0.4091 Lv:1.4503\n",
    "\n",
    "\n",
    "# Training       : 100%|██████████| 1804/1804 [19:28<00:00,  1.54it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [07:31<00:00,  1.85it/s]\n",
    "# Epoch-001: Lt:0.3276 Lv:1.1352\n",
    "\n",
    "\n",
    "# Training       : 100%|██████████| 1804/1804 [19:20<00:00,  1.55it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [07:06<00:00,  1.96it/s]\n",
    "# Epoch-002: Lt:0.2901 Lv:1.0932\n",
    "\n",
    "\n",
    "# Training       : 100%|██████████| 1804/1804 [19:16<00:00,  1.56it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [06:57<00:00,  2.00it/s]\n",
    "# Epoch-003: Lt:0.2891 Lv:0.7242\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize normals\n",
    "# proj = train_ds.projects[7]\n",
    "for i, proj in enumerate(train_ds.projects):\n",
    "    PlyData([PlyElement.describe(pd.DataFrame(np.concatenate([proj['xyz'],proj['normals'],np.squeeze(proj['incAngles'])[:,None],np.proj['distanceFromScanner'][:,None], proj['sop_ids'][:,None]], axis=1),columns=['x','y','z','nx','ny','nz','incAngle','distanceFromScanner','sop_ids']).to_records(index=False),'vertex')]).write(f\"./_temp/proj_{i:03}.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in [proj['xyz'],proj['normals'],np.squeeze(proj['incAngles'])[:,None],proj['distanceFromScanner'][:,None], proj['sop_ids'][:,None]]:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01236105, 0.0084599 , 0.00521252, 0.02295678, 0.02016745,\n",
       "       0.0234277 , 0.19764559, 0.15461153, 0.00264833, 0.        ,\n",
       "       0.00000009, 0.44791493, 0.09292256, 0.04890899, 0.09555608,\n",
       "       0.11370969], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "train_ds[0]['features_neibors'].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01326171, 0.00831641, 0.00541755, 0.09005746, 0.07818554,\n",
       "       0.07874302, 0.19764559, 0.15461153, 0.00264833, 0.        ,\n",
       "       0.00000009, 0.4461691 , 0.09488796, 0.05305372, 0.08529734,\n",
       "       0.10812434], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "train_ds[0]['features_neibors'].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj['incAngles'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj['incAngles'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj['distanceFromScanner'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(proj['distanceFromScanner']-proj['distanceFromScanner'].mean()).std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
