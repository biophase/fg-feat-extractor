{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_projects = [ # total: 13 point clouds (including bbox splitting)\n",
    "    # {\"proj_name\": \"2023-08-28_FW_EingangBauing.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-03-22_FW_Koenigshuegel.FwfProj\", \"bboxes\" : []},                      # building\n",
    "    # {\"proj_name\": \"2024-04-05_FW_Westbahnhof_01.FwfProj\", \"bboxes\" : []},                   # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_02.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_03.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_04.FwfProj\", \"bboxes\" : []},                     # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_05.FwfProj\", \"bboxes\" : []},                     # building\n",
    "    {\"proj_name\": \"2024-05-10_FW_RWTH_Zentrum_01.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Koenigstr.FwfProj\", \"bboxes\" : [0,2]},               # bridge (steel+stone)\n",
    "    # {\"proj_name\": \"2024-07-31_FW_Bruecke_Turmstr.FwfProj\", \"bboxes\" : []},                  # bridge (concrete/steel)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj\", \"bboxes\" : []},   # bridge (concrete)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Deltourserb.FwfProj\", \"bboxes\" : []},                # bridge (concrete)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj\", \"bboxes\" : [1]},             # bridge (concrete, steel, brick)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_RotheErde.FwfProj\", \"bboxes\" : []},                  # bridge (steel)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Rottstrasse.FwfProj\", \"bboxes\" : []},                # bridge (concrete)\n",
    "]\n",
    "val_projects = [ # total: 5 point clouds (including bbox splitting)\n",
    "    {\"proj_name\": \"2023-08-28_FW_EingangBauing.FwfProj\", \"bboxes\" : []},                    # building\n",
    "    {\"proj_name\": \"2024-04-05_FW_Westbahnhof_01.FwfProj\", \"bboxes\" : []},                   # tunnel-bridge\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Koenigstr.FwfProj\", \"bboxes\" : [1]},               # bridge (steel+stone)\n",
    "    {\"proj_name\": \"2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj\", \"bboxes\" : [0]},           # bridge (concrete, steel, brick)\n",
    "    {\"proj_name\": \"2024-07-31_FW_Bruecke_Turmstr.FwfProj\", \"bboxes\" : []},                  # bridge (concrete/steel)\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BBox:\n",
    "    \"\"\"Bounding box using the CloudCompare synthax\n",
    "    \"\"\"\n",
    "    def __init__(self, orientation:np.ndarray, width: np.ndarray) -> None:\n",
    "        # an easy way to retrieve these values is by using the cross-section tool in cloudcompare\n",
    "        # so cross-section -> advanced -> orientation -> to clipboard\n",
    "        self.orientation = orientation\n",
    "        # from the same menu copy and paste the values under 'width' \n",
    "        self.width = width\n",
    "        \n",
    "    def cutout(self, xyz:np.ndarray) -> np.ndarray:\n",
    "            # Inverse of the orientation matrix (rotation and translation)\n",
    "            orientation_inv = np.linalg.inv(self.orientation)\n",
    "            \n",
    "            # Transform points into the bbox's local coordinate system\n",
    "            transformed_xyz = (orientation_inv @ np.vstack((xyz.T, np.ones(xyz.shape[0])))).T\n",
    "            \n",
    "            # Get the half-dimensions of the bbox (since width is the full length)\n",
    "            half_width = self.width / 2\n",
    "            \n",
    "            # Check if points lie within the bbox in local coordinates\n",
    "            inside_bbox = (\n",
    "                (np.abs(transformed_xyz[:, 0]) <= half_width[0]) &\n",
    "                (np.abs(transformed_xyz[:, 1]) <= half_width[1]) &\n",
    "                (np.abs(transformed_xyz[:, 2]) <= half_width[2])\n",
    "            )\n",
    "            \n",
    "            # Return the indices of the points inside the bbox\n",
    "            return np.where(inside_bbox)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pointcloud import grid_subsample\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def grid_subsample_simple(xyz:np.ndarray, voxel_size:float, device='cuda'):\n",
    "    \"\"\"\n",
    "    Wrapper using numpy arrays\n",
    "    \"\"\"\n",
    "    sub = grid_subsample(torch.Tensor(xyz).to(device=device), voxel_size) # n -> m number of points\n",
    "    sub['points'] = sub['points'].to(dtype=torch.float64).detach().cpu().numpy()\n",
    "    sub['inv_inds'] = sub['inv_inds'].detach().cpu().numpy()\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple, List\n",
    "import colorsys\n",
    "\n",
    "class Transform():\n",
    "    def __init__(self, fn:Callable = lambda x: x):\n",
    "        self.fn = fn\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.fn(x)\n",
    "\n",
    "class TransZRotation(Transform):\n",
    "    def __init__(self, limits:Tuple = (0, 2*np.pi)):\n",
    "        def fn(x):\n",
    "            r = np.random.uniform(*limits)\n",
    "            rot_matrix = np.array([[np.cos(r), -np.sin(r), 0],\n",
    "                                   [np.sin(r),  np.cos(r), 0],\n",
    "                                   [0,          0,         1]])\n",
    "            return x @ rot_matrix\n",
    "            \n",
    "        super().__init__(fn)\n",
    "\n",
    "\n",
    "class TransScaling(Transform):\n",
    "    def __init__(self, limits: Tuple = (0.85, 1.15)):\n",
    "        def fn(x):\n",
    "            scale_factor = np.random.uniform(*limits)\n",
    "            scale_matrix = np.diag([scale_factor, scale_factor, scale_factor])  # Uniform scaling\n",
    "            return x @ scale_matrix\n",
    "        \n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransGaussianNoise(Transform):\n",
    "    def __init__(self, mean: float = 0.0, std: float = 0.01):\n",
    "        def fn(x:np.ndarray):\n",
    "            noise = np.random.normal(loc=mean, scale=std, size=x.shape)\n",
    "            return x + noise\n",
    "\n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransGammaCorrection(Transform):\n",
    "    def __init__(self, limits: Tuple[float, float] = (0.8, 1.2)):\n",
    "        def fn(x):\n",
    "            gamma = np.random.uniform(*limits)\n",
    "            return np.clip(x ** gamma, 0, 1) # TODO: Check order of normalization and augmentation !!!\n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransSignalScaling(Transform):\n",
    "    def __init__(self,limits:Tuple[float,float] = (0.95,1.05)):\n",
    "        def fn(x:np.ndarray):\n",
    "            # waveform shape is neibors, 32\n",
    "            scale_factors = np.random.uniform(*limits, size=(x.shape[0],1))\n",
    "            return x * scale_factors\n",
    "        \n",
    "        super().__init__(fn)\n",
    "        \n",
    "class TransFeatureDropout(Transform):\n",
    "    def __init__(self, dropout_prob: float = 0.2):\n",
    "\n",
    "        def fn(x):\n",
    "            mask = np.random.binomial(1, 1 - dropout_prob, size=x.shape[1])\n",
    "            return x * mask  \n",
    "        \n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransStandardize(Transform):\n",
    "    def __init__(self, mean, std):\n",
    "        def fn(x:np.ndarray):\n",
    "            x -= mean\n",
    "            x /= std\n",
    "            return x\n",
    "        super().__init__(fn)\n",
    "\n",
    "class TransformsList():\n",
    "    def __init__(self,\n",
    "                 transforms:List[Transform]):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for t in self.transforms:\n",
    "            x = t(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_idx = 56\n",
    "# proj = train_ds.projects[0]\n",
    "# neibs = proj['neibors'][res_idx]\n",
    "# rgb = proj['rgb'][neibs]\n",
    "\n",
    "\n",
    "# transform = TransformsList([\n",
    "#     #  TransGammaCorrection(),\n",
    "#     TransStandardize(mean=np.array([[0.29689665, 0.3428666,  0.190237]]),std=np.array([[0.21558372, 0.23351644, 0.21213871]])),\n",
    "#     TransGaussianNoise(0,0.02),\n",
    "#     TransFeatureDropout(dropout_prob=0.1)                         \n",
    "# ])\n",
    "\n",
    "# print(rgb.shape, rgb.mean(), rgb.std())\n",
    "# rgb = transform(rgb)\n",
    "# print(rgb.shape, rgb.mean(), rgb.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from scipy.spatial import KDTree\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from jakteristics import compute_features\n",
    "from typing import List, Optional, Dict\n",
    "import torch\n",
    "\n",
    "from utils.pointcloud import grid_subsample\n",
    "\n",
    "class FwfDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 proj_search_pattern,\n",
    "                 proj_query_list,\n",
    "                 return_fields_input = [\n",
    "                    'xyz',\n",
    "                    'rgb',\n",
    "                    'riegl_feats',\n",
    "                    'normals',\n",
    "                    'incAngles',\n",
    "                    'distanceFromScanner',\n",
    "                ],\n",
    "                 label_names = None,\n",
    "                 return_waveform = True,\n",
    "                 query_grid_size = None,\n",
    "                 subsample_on_gpu = True,\n",
    "                 transforms_dict: Dict[str, TransformsList] = dict(\n",
    "                     xyz = TransformsList([\n",
    "                         TransZRotation(),\n",
    "                         TransScaling(),\n",
    "                         TransGaussianNoise(0,0.002)\n",
    "                     ]),\n",
    "                     rgb = TransformsList([\n",
    "                        #  TransGammaCorrection(),\n",
    "                        #  TransStandardize(mean=np.array([[0.29689665, 0.3428666,  0.190237]]),std=np.array([[0.21558372, 0.23351644, 0.21213871]])),\n",
    "                         TransGaussianNoise(0,0.02),\n",
    "                         TransFeatureDropout(dropout_prob=0.1)                         \n",
    "                     ]),\n",
    "                     wfm = TransformsList([\n",
    "                        # TransStandardize(mean=358.35934, std=623.0141),\n",
    "                        TransSignalScaling(),\n",
    "                     ]),\n",
    "                     normals = TransformsList([\n",
    "                         TransGaussianNoise(0,0.05)\n",
    "                     ]),\n",
    "                     incAngles = TransformsList([\n",
    "                         TransGaussianNoise(0,0.05),\n",
    "                         TransStandardize(mean = 2.07, std = 0.38),\n",
    "                     ]),\n",
    "                     distanceFromScanner = TransformsList([\n",
    "                         TransGaussianNoise(0,0.1),\n",
    "                         TransStandardize(mean = 12.13, std = 12.20),\n",
    "                     ]),\n",
    "                    #  riegl_feats = TransformsList([\n",
    "                    #      TransStandardize(mean=np.array([[-6.90191949, 25.16398933, 26.45952891,  1.,  1.03636612]]), std= np.array([[2.32590898, 2.98518547, 929.71399545, 1., 0.22651793]]))\n",
    "                    #  ])\n",
    "                 )\n",
    "                 ):\n",
    "        \n",
    "        super(Dataset,self).__init__()\n",
    "        \n",
    "        self.proj_query_list = proj_query_list\n",
    "        self.return_fields_input = return_fields_input\n",
    "        self.return_waveform = return_waveform\n",
    "        self.transforms_dict = transforms_dict\n",
    "\n",
    "        self.projects = list()\n",
    "        \n",
    "        # keep track of point cloud sizes\n",
    "        self.proj_lens = []\n",
    "\n",
    "        \n",
    "        counter = 0\n",
    "        # go through search pattern\n",
    "        for fwf_prof_fp in glob(proj_search_pattern):\n",
    "            proj_name = os.path.basename(fwf_prof_fp)\n",
    "            \n",
    "            # only get projects if they are on the project list\n",
    "            if proj_name not in [p['proj_name'] for p in self.proj_query_list]:\n",
    "                continue\n",
    "            else:\n",
    "                bbox_queries = [p['bboxes'] for p in self.proj_query_list if p['proj_name']==proj_name][0]\n",
    "\n",
    "                \n",
    "            print(f\"Loading '{proj_name}'; Bounding box IDs = {bbox_queries if len(bbox_queries) else 'default'}\")\n",
    "\n",
    "            # load the data\n",
    "            pcd = pd.DataFrame(PlyData.read(list(glob(os.path.join(fwf_prof_fp,'labeled','*pointcloud.ply')))[0]).elements[0].data)\n",
    "            wfm = np.load(list(glob(os.path.join(fwf_prof_fp,'labeled','*waveform.npy')))[0]).astype(np.float32)\n",
    "            meta = json.load(open(list(glob(os.path.join(fwf_prof_fp,'labeled','*metadata.json')))[0],\"r\"))\n",
    "            \n",
    "            # field names (constants)\n",
    "            riegl_feat_names = [ 'riegl_reflectance','riegl_amplitude', 'riegl_deviation', 'riegl_targetIndex','riegl_targetCount']\n",
    "            # label_names = ['labels_0', 'labels_1', 'labels_2', 'labels_3']\n",
    "            self.label_names:Optional[List[str]] = label_names\n",
    "            \n",
    "            \n",
    "            # convert to numpy\n",
    "            riegl_feats = pcd[riegl_feat_names].to_numpy()\n",
    "            rgb = pcd[['Red','Green','Blue']].to_numpy()\n",
    "            \n",
    "            # normalize the data\n",
    "            # FIXME: Statistics calculated on first point cloud only\n",
    "            wfm = (wfm.astype(np.float32) - 358.35934) / 623.0141 # - mean / std\n",
    "\n",
    "            riegl_feats -= np.array([[-6.90191949, 25.16398933, 26.45952891,  1.        ,  1.03636612]]) # - means\n",
    "            riegl_feats /= np.array([[  2.32590898, 2.98518547, 929.71399545, 1., 0.22651793]]) # /std\n",
    "            \n",
    "            rgb -= np.array([[0.29689665, 0.3428666,  0.190237]]) # means\n",
    "            rgb /= np.array([[0.21558372, 0.23351644, 0.21213871]]) # std          \n",
    "            \n",
    "\n",
    "            \n",
    "            # get scan positions\n",
    "            if 'scanId=000' in meta['scan_positions'].keys():\n",
    "                sop = np.array([meta['scan_positions'][f'scanId={si:03}']['sop'] for si in range(len(meta['scan_positions']))])\n",
    "            else:\n",
    "                # handle case where only one scan position is in the metadata and it's f.s.r. labeled 'scanId=001' instead of 'scanID=000'\n",
    "                sop = np.array([meta['scan_positions']['scanId=001']['sop']])\n",
    "\n",
    "            # get full xyz\n",
    "            xyz_defaultBbox = pcd[['x','y','z']].to_numpy()       \n",
    "\n",
    "                \n",
    "            # save project as dict\n",
    "            if len(bbox_queries)==0:\n",
    "                # handle default case\n",
    "                sub = grid_subsample_simple(xyz_defaultBbox,query_grid_size, 'cuda' if subsample_on_gpu else 'cpu')\n",
    "                kd_tree = KDTree(xyz_defaultBbox)\n",
    "                _, sub_ids = kd_tree.query(sub['points'])\n",
    "                self.projects.append(dict(\n",
    "                    proj_name=f\"{proj_name}::defaultBbox\",\n",
    "                    xyz = xyz_defaultBbox,\n",
    "                    wfm = wfm,\n",
    "                    sop = sop,\n",
    "                    rgb = rgb,\n",
    "                    riegl_feats = riegl_feats,\n",
    "                    labels = pcd[self.label_names].to_numpy()[sub_ids], # labels need to be subsampeld \n",
    "                    sop_ids = pcd['scan_id'].to_numpy(),\n",
    "                    kd_tree = kd_tree,\n",
    "                    xyz_sub = sub['points'],\n",
    "                    sub_inv = sub['inv_inds']\n",
    "                ))\n",
    "                self.proj_lens.append(sub['points'].shape[0])\n",
    "            else:\n",
    "                # handle region bboxes case\n",
    "                for bbox_i in bbox_queries:\n",
    "                    bbox_meta = meta['bboxes'][f'bboxId={bbox_i:03}']\n",
    "                    bbox = BBox(orientation=np.array(bbox_meta['orientation']), width=np.array(bbox_meta['width']))\n",
    "                    subcloud_mask=bbox.cutout(xyz_defaultBbox)\n",
    "                    xyz_masked = xyz_defaultBbox[subcloud_mask]\n",
    "                    sub = grid_subsample_simple(xyz_masked,query_grid_size, 'cuda' if subsample_on_gpu else 'cpu')\n",
    "                    kd_tree = KDTree(xyz_masked)\n",
    "                    _, sub_ids = kd_tree.query(sub['points'])\n",
    "                    self.projects.append(dict(\n",
    "                        proj_name=f\"{proj_name}::bboxId={bbox_i:03}\",\n",
    "                        xyz = xyz_masked,\n",
    "                        wfm = wfm[subcloud_mask],\n",
    "                        sop = sop,\n",
    "                        rgb = rgb[subcloud_mask],\n",
    "                        riegl_feats = riegl_feats[subcloud_mask],\n",
    "                        labels = pcd[self.label_names].to_numpy()[subcloud_mask][sub_ids], # labels need to be subsampeld\n",
    "                        sop_ids = pcd['scan_id'].to_numpy()[subcloud_mask],\n",
    "                        kd_tree = KDTree(xyz_masked),\n",
    "                        xyz_sub = sub['points'],\n",
    "                        sub_inv = sub['inv_inds']\n",
    "                    ))\n",
    "                    self.proj_lens.append(sub['points'].shape[0])\n",
    "                    \n",
    "            # calculate the cumulative sum of the point cloud sizes\n",
    "\n",
    "            if counter >= 3:\n",
    "                continue\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "        self.proj_lens_cumsum = np.cumsum(self.proj_lens)\n",
    "    \n",
    "    def compute_neibors_knn(self, k:int):\n",
    "        for proj in self.projects:\n",
    "            print(f\"Computing neibors for '{proj['proj_name']}' @ k={k}\")\n",
    "            dists, neib_ids = proj['kd_tree'].query(proj['xyz_sub'], k)\n",
    "            proj['neibors'] = neib_ids\n",
    "    \n",
    "\n",
    "   \n",
    "    def compute_normals_knn(self):\n",
    "        for proj in self.projects:\n",
    "            k = proj['neibors'].shape[1]\n",
    "            print(f\"Computing normals for '{proj['proj_name']}' @ k={k}\")\n",
    "            neibs_xyz = proj['xyz'][proj['neibors']]\n",
    "\n",
    "            means = neibs_xyz.mean(axis=1, keepdims=True)\n",
    "            neibs_xyz -= means\n",
    "            cov = (neibs_xyz.transpose([0,2,1]) @ neibs_xyz) / (k-1)\n",
    "            eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "            # get non-flipped normals\n",
    "            normals = eigenvecs[:, :, 0]\n",
    "\n",
    "            # upsample normals to full resolution\n",
    "            normals = normals[proj['sub_inv']]\n",
    "            \n",
    "            # move all points to scanner CS\n",
    "            points_origin_scanPos = proj['sop'][proj['sop_ids']][:,:3,3]\n",
    "            xyz_scannerCs = proj['xyz'] - points_origin_scanPos\n",
    "            signs = np.sign(np.squeeze(xyz_scannerCs[:,None,:] @ normals [:,:,None])) * -1\n",
    "            normals *= signs[:,None]\n",
    "            \n",
    "            proj['normals'] = normals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    # def compute_normals_spherical_subsample(self, r = 0.08, voxel_size = 0.03, device='cuda'):\n",
    "        \n",
    "    #     for proj in self.projects:\n",
    "    #         print(f\"Computing normals for '{proj['proj_name']}' @ r={r:.3f}m and voxel_size = {voxel_size:.3f}\")\n",
    "    #         print(\"\\tDEBUG: subsampling\")\n",
    "    #         sub = grid_subsample(torch.Tensor(proj['xyz']).to(device=device), voxel_size) # n -> m number of points\n",
    "    #         sub['points'] = sub['points'].to(dtype=torch.float64).detach().cpu().numpy()\n",
    "    #         sub['inv_inds'] = sub['inv_inds'].detach().cpu().numpy()\n",
    "    #         _,sub_ids = proj['kd_tree'].query(sub['points'])\n",
    "    #         print(\"\\tDEBUG: computing feats\")\n",
    "    #         sub_normals = compute_features(sub['points'],search_radius=0.05,feature_names=['nx','ny','nz'])\n",
    "\n",
    "    #         # flip the normals\n",
    "    #         print(\"\\tDEBUG: flipping the subsampled normals\")\n",
    "    #         sub_xyz_scannerCs = proj['xyz'][sub_ids] - proj['sop'][proj['sop_ids']][:,3,:3][sub_ids]\n",
    "    #         sub_signs = np.sign(np.squeeze(sub_xyz_scannerCs[:,None,:] @ sub_normals[:,:,None]))*-1\n",
    "    #         sub_normals *= sub_signs[:,None]\n",
    "\n",
    "    #         print(\"\\tDEBUG: reprojecting\")\n",
    "    #         proj['normals'] = sub_normals[sub['inv_inds']]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # def compute_normals_spherical(self, r = 0.08):\n",
    "    #     for proj in self.projects:\n",
    "    #         print(f\"Computing normals for '{proj['proj_name']}' @ r={r:.3f}m\")\n",
    "            \n",
    "    #         # get non-oriented normals\n",
    "    #         normals = compute_features(proj['xyz'],search_radius=0.05,feature_names=['nx','ny','nz'])\n",
    "    #         # orient the normals to corresponding scanner position\n",
    "    #         xyz_scannerCs = proj['xyz'] - proj['sop'][proj['sop_ids']][:,3,:3]\n",
    "    #         signs = np.sign(np.squeeze(xyz_scannerCs[:,None,:] @ normals [:,:,None])) * -1\n",
    "    #         normals *= signs[:,None]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def compute_incAngles(self):\n",
    "        for proj in self.projects:\n",
    "            print(f\"Computing incidence angles for '{proj['proj_name']}'\")\n",
    "            xyz_scannerCs = proj['xyz'] - proj['sop'][proj['sop_ids']][:,:3,3]\n",
    "            proj['incAngles']= np.arccos(np.squeeze((proj['normals'][:,None,:] @ xyz_scannerCs[...,None])) / \\\n",
    "                (np.linalg.norm(xyz_scannerCs,axis=-1) * np.linalg.norm(proj['normals'],axis=-1)))\n",
    "            proj['distanceFromScanner'] = np.linalg.norm(xyz_scannerCs, axis=-1)\n",
    "            \n",
    "            # reshape by adding an additional axis\n",
    "            proj['incAngles'] = proj['incAngles'][:,None]\n",
    "            proj['distanceFromScanner'] = proj['distanceFromScanner'][:,None]\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get proj index first\n",
    "        proj_idx = np.argwhere(index<self.proj_lens_cumsum)[0][0]\n",
    "        size_prev = self.proj_lens_cumsum[proj_idx-1] if proj_idx > 0 else 0\n",
    "        residual_idx = index - size_prev\n",
    "        \n",
    "        # get neibors of point at index\n",
    "        neibs = self.projects[proj_idx]['neibors'][residual_idx]\n",
    "        \n",
    "        \n",
    "\n",
    "        # TODO: remove the default option (instead putting the fields as a default argument list to constructor)\n",
    "\n",
    "            # features_neibors = np.concatenate([\n",
    "            #     self.projects[proj_idx]['xyz'][neibs],\n",
    "            #     self.projects[proj_idx]['rgb'][neibs],\n",
    "            #     self.projects[proj_idx]['riegl_feats'][neibs],\n",
    "            #     self.projects[proj_idx]['normals'][neibs],\n",
    "            #     self.projects[proj_idx]['incAngles'][neibs],\n",
    "            #     self.projects[proj_idx]['distanceFromScanner'][neibs],\n",
    "            # ], axis=-1).astype(np.float32),\n",
    "            # features_point = np.concatenate([\n",
    "            #     self.projects[proj_idx]['xyz'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['rgb'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['riegl_feats'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['normals'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['incAngles'][residual_idx][None,:],\n",
    "            #     self.projects[proj_idx]['distanceFromScanner'][residual_idx][None,:],\n",
    "            # ],axis=-1).astype(np.float32)\n",
    "\n",
    "\n",
    "        return_dict = dict(\n",
    "            features_neibors = np.concatenate(\n",
    "                [\n",
    "                    self.transforms_dict[f](self.projects[proj_idx][f][neibs])\n",
    "                    if f in self.transforms_dict else self.projects[proj_idx][f][neibs]\n",
    "                    for f in self.return_fields_input\n",
    "                ],\n",
    "                axis=-1\n",
    "            ).astype(np.float32),\n",
    "            features_point = np.concatenate(\n",
    "                [\n",
    "                    self.transforms_dict[f](self.projects[proj_idx][f][residual_idx][None,:])\n",
    "                    if f in self.transforms_dict else self.projects[proj_idx][f][residual_idx][None,:]\n",
    "                    for f in self.return_fields_input\n",
    "                ],\n",
    "                axis=-1\n",
    "            ).astype(np.float32)\n",
    "        )\n",
    "\n",
    "        # add waveforms and labels\n",
    "        return_dict.update(dict(\n",
    "            wfm_neibors = self.projects[proj_idx]['wfm'][neibs] if self.return_waveform else None,\n",
    "            wfm_point = self.projects[proj_idx]['wfm'][residual_idx][None,:] if self.return_waveform else None,\n",
    "            labels = self.projects[proj_idx]['labels'][residual_idx],\n",
    "        )) # type:ignore\n",
    "        \n",
    "\n",
    "        \n",
    "        return return_dict\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.sum([p['xyz_sub'].shape[0] for p in self.projects])\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FGFeatNetwork (nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_input_feats,\n",
    "                 label_structure = {'labels_0':3, 'labels_1':10, 'labels_2':12, 'labels_3':18},\n",
    "                 dropout_prob = 0.3,\n",
    "                 global_constraint = True,\n",
    "                 ):\n",
    "        super(FGFeatNetwork, self).__init__()\n",
    "        \n",
    "        self.label_structure = label_structure\n",
    "        self.global_constraint = global_constraint\n",
    "        # Pointwise feats\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(num_input_feats, 64), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 128), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(128, 128), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "        self.wf_conv = nn.Sequential(\n",
    "            # FIXME: check conv and maxpool kernel sizes and strides\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3), nn.RReLU(),nn.BatchNorm1d(16), nn.MaxPool1d(2), # activations 32 -> 16\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3), nn.RReLU(),nn.BatchNorm1d(32), nn.MaxPool1d(2),# activations 16 -> 8\n",
    "            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3), nn.RReLU(),nn.BatchNorm1d(32), nn.MaxPool1d(2),# activations 8 -> 4\n",
    "            # after concat. activation shape = 4 * 64 = 256\n",
    "        )\n",
    "        # TODO: Try out inverse bottleneck?\n",
    "        # TODO: Network size might be an overkill / unfeasable for the task\n",
    "        \n",
    "        # MLP after concat with WFM feats\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(192, 512), nn.ReLU(), nn.Dropout(p=dropout_prob), \n",
    "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(704, 512), nn.ReLU(), nn.Dropout(p=dropout_prob),  # joined shape (point + neibors)\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(p=dropout_prob),\n",
    "        )\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.ModuleDict({k:nn.Linear(128,v) for k,v in self.label_structure.items()})\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # handle neibor features\n",
    "        pw_feats_neib = self.mlp1(x['features_neibors'])\n",
    "        batch, neibor, signal_length = x['wfm_neibors'].shape\n",
    "        wf_feats_neib = self.wf_conv(x['wfm_neibors'].view((batch * neibor, 1, signal_length))) # batch, neibor, signal_length -> batch * neibor, 1,  signal_length\n",
    "        wf_feats_neib = wf_feats_neib.view((batch, neibor,-1))\n",
    "        joined_feats_neib = torch.cat([pw_feats_neib, wf_feats_neib], dim=-1)\n",
    "        \n",
    "        # handle the point features\n",
    "        pw_feats_point = self.mlp1(x['features_point'])\n",
    "        batch, neibor, signal_length = x['wfm_point'].shape\n",
    "        wf_feats_point = self.wf_conv(x['wfm_point'].view((batch * neibor, 1, signal_length))) ## batch, 1, signal_length -> batch , 1,  signal_length\n",
    "        wf_feats_point = wf_feats_point.view((batch, neibor,-1))\n",
    "        joined_feats_point = torch.cat([pw_feats_point, wf_feats_point], dim=-1)        \n",
    "        \n",
    "        # continue processing neibor feats\n",
    "        \n",
    "        joined_feats_neib = self.mlp2(joined_feats_neib)\n",
    "        joined_feats_neib = torch.max(joined_feats_neib, dim=1)[0]\n",
    "        \n",
    "        # join neibor with skip connection to point features\n",
    "        global_feat = torch.cat([joined_feats_neib, torch.squeeze(joined_feats_point)],dim=-1)\n",
    "        if self.global_constraint:\n",
    "            global_feat = global_feat / (torch.norm(global_feat, p=2, dim=1, keepdim=True)+1e-6)\n",
    "\n",
    "        global_feat = self.mlp3(global_feat)\n",
    "        \n",
    "        result = {k: self.classifier[k](global_feat) for k in self.classifier.keys()}\n",
    "        \n",
    "        \n",
    "        \n",
    "        return result\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '2024-03-22_FW_Koenigshuegel.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_02.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_03.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_04.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_05.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-05-10_FW_RWTH_Zentrum_01.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-07-31_FW_Bruecke_Koenigstr.FwfProj'; Bounding box IDs = [0, 2]\n",
      "Loading '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Deltourserb.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj'; Bounding box IDs = [1]\n",
      "Loading '2024-08-02_FW_Bruecke_RotheErde.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj'; Bounding box IDs = default\n",
      "Loading '2023-08-28_FW_EingangBauing.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-04-05_FW_Westbahnhof_01.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-07-31_FW_Bruecke_Koenigstr.FwfProj'; Bounding box IDs = [1]\n",
      "Loading '2024-07-31_FW_Bruecke_Turmstr.FwfProj'; Bounding box IDs = default\n",
      "Loading '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj'; Bounding box IDs = [0]\n",
      "Computing neibors for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=20\n",
      "Computing incidence angles for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox'\n",
      "Computing neibors for '2024-03-22_FW_Koenigshuegel.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_02.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_03.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_04.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_05.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-05-10_FW_RWTH_Zentrum_01.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=000' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=002' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_A44_VerlautenheidenerStr.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Deltourserb.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=001' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_RotheErde.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Rottstrasse.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001' @ k=20\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox' @ k=20\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000' @ k=20\n",
      "Computing normals for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001' @ k=20\n",
      "Computing normals for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox' @ k=20\n",
      "Computing normals for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000' @ k=20\n",
      "Computing incidence angles for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001'\n",
      "Computing incidence angles for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox'\n",
      "Computing incidence angles for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000'\n",
      "Computing neibors for '2023-08-28_FW_EingangBauing.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-04-05_FW_Westbahnhof_01.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Koenigstr.FwfProj::bboxId=001' @ k=96\n",
      "Computing neibors for '2024-07-31_FW_Bruecke_Turmstr.FwfProj::defaultBbox' @ k=96\n",
      "Computing neibors for '2024-08-02_FW_Bruecke_Kasinostrasse.FwfProj::bboxId=000' @ k=96\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "device='cuda'\n",
    "label_names = [\n",
    "    'labels_0',\n",
    "    # 'labels_1',\n",
    "    # 'labels_2',\n",
    "    # 'labels_3',\n",
    "]\n",
    "query_grid_size = 0.1\n",
    "num_neib_normals = 20\n",
    "num_neib_featextraction = 96\n",
    "\n",
    "\n",
    "\n",
    "# create Dataset\n",
    "proj_search_pattern = r\"D:\\Projekte\\GIA_220412_PCS\\02_Datasets\\FullWaveForm\\full_waveform_software\\FullWaveformParse_mass\\output\\FWF_Aachen_labeled\\*.FwfProj\" # work pc\n",
    "# proj_search_pattern = \"../../02_Datasets/FWF_Aachen_labeled/*.FwfProj\" # home pc\n",
    "train_ds = FwfDataset(\n",
    "    proj_search_pattern=proj_search_pattern, \n",
    "    proj_query_list=train_projects, \n",
    "    query_grid_size=query_grid_size,\n",
    "    label_names = label_names,\n",
    "    return_waveform = True,\n",
    "    subsample_on_gpu = True\n",
    ")\n",
    "val_ds = FwfDataset(\n",
    "    proj_search_pattern=proj_search_pattern, \n",
    "    proj_query_list=val_projects, \n",
    "    query_grid_size=query_grid_size,\n",
    "    label_names = label_names,\n",
    "    return_waveform = True,\n",
    "    subsample_on_gpu = True\n",
    ")\n",
    "\n",
    "# rest computation\n",
    "for ds in [train_ds, val_ds]:\n",
    "    ds.compute_neibors_knn(k=num_neib_normals)\n",
    "    ds.compute_normals_knn()\n",
    "    ds.compute_incAngles()\n",
    "    ds.compute_neibors_knn(k=num_neib_featextraction)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = FGFeatNetwork(\n",
    "    num_input_feats = train_ds[0]['features_neibors'].shape[-1],\n",
    "    label_structure = {\n",
    "        \"labels_0\":3,\n",
    "        # \"labels_1\":8,\n",
    "        # \"labels_2\":18,\n",
    "        # \"labels_3\":18\n",
    "    },\n",
    "    global_constraint=True\n",
    "    ).to(device=device)\n",
    "\n",
    "# create dataloader\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training       : 100%|██████████| 57700/57700 [25:18<00:00, 37.99it/s]\n",
      "Validation     : 100%|██████████| 26692/26692 [09:03<00:00, 49.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-000: Lt:0.4103 Lv:5.7524\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training       : 100%|██████████| 57700/57700 [26:13<00:00, 36.67it/s] \n",
      "Validation     : 100%|██████████| 26692/26692 [09:07<00:00, 48.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-001: Lt:0.3975 Lv:7.0679\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training       : 100%|██████████| 57700/57700 [25:35<00:00, 37.58it/s] \n",
      "Validation     : 100%|██████████| 26692/26692 [08:46<00:00, 50.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-002: Lt:0.3930 Lv:7.6406\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training       :  48%|████▊     | 27543/57700 [13:06<14:20, 35.03it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# put batch on device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Hristo\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mFwfDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    279\u001b[0m neibs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneibors\u001b[39m\u001b[38;5;124m'\u001b[39m][residual_idx]\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: remove the default option (instead putting the fields as a default argument list to constructor)\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# features_neibors = np.concatenate([\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m#     self.projects[proj_idx]['distanceFromScanner'][residual_idx][None,:],\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m# ],axis=-1).astype(np.float32)\u001b[39;00m\n\u001b[0;32m    303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    304\u001b[0m     features_neibors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[1;32m--> 305\u001b[0m         [\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict[f](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][neibs])\n\u001b[0;32m    307\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][neibs]\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_fields_input\n\u001b[0;32m    309\u001b[0m         ],\n\u001b[0;32m    310\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    311\u001b[0m     )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    312\u001b[0m     features_point \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    313\u001b[0m         [\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict[f](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][residual_idx][\u001b[38;5;28;01mNone\u001b[39;00m,:])\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][residual_idx][\u001b[38;5;28;01mNone\u001b[39;00m,:]\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_fields_input\n\u001b[0;32m    317\u001b[0m         ],\n\u001b[0;32m    318\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    319\u001b[0m     )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# add waveforms and labels\u001b[39;00m\n\u001b[0;32m    323\u001b[0m return_dict\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    324\u001b[0m     wfm_neibors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwfm\u001b[39m\u001b[38;5;124m'\u001b[39m][neibs] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_waveform \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m     wfm_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwfm\u001b[39m\u001b[38;5;124m'\u001b[39m][residual_idx][\u001b[38;5;28;01mNone\u001b[39;00m,:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_waveform \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    326\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][residual_idx],\n\u001b[0;32m    327\u001b[0m )) \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    279\u001b[0m neibs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneibors\u001b[39m\u001b[38;5;124m'\u001b[39m][residual_idx]\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# TODO: remove the default option (instead putting the fields as a default argument list to constructor)\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# features_neibors = np.concatenate([\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m#     self.projects[proj_idx]['distanceFromScanner'][residual_idx][None,:],\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m# ],axis=-1).astype(np.float32)\u001b[39;00m\n\u001b[0;32m    303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    304\u001b[0m     features_neibors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    305\u001b[0m         [\n\u001b[1;32m--> 306\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mproj_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mneibs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][neibs]\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_fields_input\n\u001b[0;32m    309\u001b[0m         ],\n\u001b[0;32m    310\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    311\u001b[0m     )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    312\u001b[0m     features_point \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    313\u001b[0m         [\n\u001b[0;32m    314\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict[f](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][residual_idx][\u001b[38;5;28;01mNone\u001b[39;00m,:])\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][f][residual_idx][\u001b[38;5;28;01mNone\u001b[39;00m,:]\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_fields_input\n\u001b[0;32m    317\u001b[0m         ],\n\u001b[0;32m    318\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    319\u001b[0m     )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# add waveforms and labels\u001b[39;00m\n\u001b[0;32m    323\u001b[0m return_dict\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    324\u001b[0m     wfm_neibors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwfm\u001b[39m\u001b[38;5;124m'\u001b[39m][neibs] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_waveform \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m     wfm_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwfm\u001b[39m\u001b[38;5;124m'\u001b[39m][residual_idx][\u001b[38;5;28;01mNone\u001b[39;00m,:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_waveform \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    326\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojects[proj_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][residual_idx],\n\u001b[0;32m    327\u001b[0m )) \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTransformsList.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 80\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTransGaussianNoise.__init__.<locals>.fn\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(x:np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m---> 35\u001b[0m     noise \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m noise\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "weight_decay = 1e-4\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "optim = Adam(params=model.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    epoch_train_loss = []\n",
    "    # train\n",
    "    for batch in tqdm(train_dl, desc=f\"{'Training':<15}\", leave=True):\n",
    "        optim.zero_grad()\n",
    "        # put batch on device\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device=device)\n",
    "\n",
    "        # forward pass\n",
    "        result = model(batch)\n",
    "        \n",
    "        # agregate loss on all levels\n",
    "        loss = torch.tensor(0.).to(device=device)\n",
    "        for k in result.keys():\n",
    "            output = result[k]\n",
    "            gt = batch['labels'][:, train_ds.label_names.index(k)] # type:ignore\n",
    "            loss += criterion(output, gt)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    # validate    \n",
    "    epoch_val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl, desc=f\"{'Validation':<15}\", leave=True):\n",
    "            # put batch on device\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device=device)\n",
    "\n",
    "            # forward pass\n",
    "            result = model(batch)\n",
    "            \n",
    "            # agregate loss on all levels\n",
    "            loss = torch.tensor(0.).to(device=device)\n",
    "            for k in result.keys():\n",
    "                output = result[k]\n",
    "                gt = batch['labels'][:, val_ds.label_names.index(k)] # type:ignore\n",
    "                loss += criterion(output, gt)\n",
    "            \n",
    "            epoch_val_loss.append(loss.item())\n",
    "    print(f\"Epoch-{epoch:03}: Lt:{np.mean(epoch_train_loss):.4f} Lv:{np.mean(epoch_val_loss):.4f}\")\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training       : 100%|██████████| 1804/1804 [19:05<00:00,  1.57it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [07:57<00:00,  1.75it/s]\n",
    "# Epoch-000: Lt:0.4091 Lv:1.4503\n",
    "\n",
    "\n",
    "# Training       : 100%|██████████| 1804/1804 [19:28<00:00,  1.54it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [07:31<00:00,  1.85it/s]\n",
    "# Epoch-001: Lt:0.3276 Lv:1.1352\n",
    "\n",
    "\n",
    "# Training       : 100%|██████████| 1804/1804 [19:20<00:00,  1.55it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [07:06<00:00,  1.96it/s]\n",
    "# Epoch-002: Lt:0.2901 Lv:1.0932\n",
    "\n",
    "\n",
    "# Training       : 100%|██████████| 1804/1804 [19:16<00:00,  1.56it/s]\n",
    "# Validation     : 100%|██████████| 835/835 [06:57<00:00,  2.00it/s]\n",
    "# Epoch-003: Lt:0.2891 Lv:0.7242\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize normals\n",
    "# proj = train_ds.projects[7]\n",
    "for i, proj in enumerate(train_ds.projects):\n",
    "    PlyData([PlyElement.describe(pd.DataFrame(np.concatenate([proj['xyz'],proj['normals'],np.squeeze(proj['incAngles'])[:,None],np.proj['distanceFromScanner'][:,None], proj['sop_ids'][:,None]], axis=1),columns=['x','y','z','nx','ny','nz','incAngle','distanceFromScanner','sop_ids']).to_records(index=False),'vertex')]).write(f\"./_temp/proj_{i:03}.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in [proj['xyz'],proj['normals'],np.squeeze(proj['incAngles'])[:,None],proj['distanceFromScanner'][:,None], proj['sop_ids'][:,None]]:\n",
    "    print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01236105, 0.0084599 , 0.00521252, 0.02295678, 0.02016745,\n",
       "       0.0234277 , 0.19764559, 0.15461153, 0.00264833, 0.        ,\n",
       "       0.00000009, 0.44791493, 0.09292256, 0.04890899, 0.09555608,\n",
       "       0.11370969], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "train_ds[0]['features_neibors'].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01326171, 0.00831641, 0.00541755, 0.09005746, 0.07818554,\n",
       "       0.07874302, 0.19764559, 0.15461153, 0.00264833, 0.        ,\n",
       "       0.00000009, 0.4461691 , 0.09488796, 0.05305372, 0.08529734,\n",
       "       0.10812434], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "train_ds[0]['features_neibors'].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj['incAngles'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj['incAngles'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj['distanceFromScanner'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(proj['distanceFromScanner']-proj['distanceFromScanner'].mean()).std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
